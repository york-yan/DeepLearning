{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "- 访问参数，便于调试诊断\n",
    "- 参数初始化\n",
    "- 不同模型之间共享参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "print(*[name for name, param in net[0].named_parameters()])\n",
    "print(net[0].state_dict()) #第一个全连接层\n",
    "print(net[1].state_dict()) #relu层\n",
    "print(net[2].state_dict()) #第二个全连接层\n",
    "print(\"------------------------------\")\n",
    "print(type(net[2].bias)) # 返回类型\n",
    "print(net[2].bias)       # 打印bias\n",
    "print(net[2].bias.data)\n",
    "print(\"==============================\")\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "rgnet=nn.Sequential(block2(),nn.Linear(4, 1))\n",
    "X=torch.rand(size=(2,4))\n",
    "rgnet(X)\n",
    "print(rgnet) #查看当前的网络结构\n",
    "print(rgnet[0][1][0].bias.data) # 通过索引访问具体的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数初始化\n",
    "好的初始化参数能够提升模型性能、增强泛化能力、提高计算效率\n",
    "\n",
    "$ y=\\sigma (wx+b) $\n",
    "\n",
    "w-权重\n",
    "b-偏置\n",
    "$\\sigma$-激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "rgnet=nn.Sequential(block2(),nn.Linear(4, 1))\n",
    "X=torch.rand(size=(2,4))\n",
    "rgnet(X)\n",
    "# print(rgnet) #查看当前的网络结构\n",
    "# print(rgnet[0][1][0].bias.data) # 通过索引访问具体的值\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.1)\n",
    "        nn.init.zeros_(m.bias)  #这里初始化为零了\n",
    "\n",
    "def init_constant(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "def my_init(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        print(\"init\",*[(name,param.shape) for name,param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight,-10,10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5 # 创建一个等大小的布尔变量 要么为0，要么就是绝对值大于5\n",
    "\n",
    "\n",
    "\n",
    "rgnet.apply(init_normal)\n",
    "print(rgnet[0][1][0].weight.data)\n",
    "print(rgnet[0][1][0].bias.data)\n",
    "\n",
    "rgnet.apply(init_constant)      # 修改为常数\n",
    "print(rgnet[0][1][0].weight.data)\n",
    "print(rgnet[0][1][0].bias.data)\n",
    "\n",
    "rgnet.apply(my_init)\n",
    "print(rgnet[0][1][0].weight.data)\n",
    "print(rgnet[0][1][0].bias.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数共享\n",
    "在多个层之间共享参数，可以定义一个稠密层，使用其参数来设置另一个层的采纳数，参数共享的好处有以下几点：\n",
    "- 节省内存\n",
    "- 对于图像识别可以全图的任何地方而不是只在特定的区域查找\n",
    "- 对于RNN适应各个时间步骤\n",
    "- 提高了泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "shared=nn.Linear(8,8)\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),shared,nn.ReLU(),shared,nn.ReLU(),nn.Linear(8,1))\n",
    "\n",
    "X=torch.rand(size=(2,4))\n",
    "print(net)\n",
    "print(net(X))\n",
    "print(net[2].weight.data==net[4].weight.data) # 判断是否相等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
